{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzKlVg/eYMqdZoGD7fzWjS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Exercise/blob/main/Corpus_Toolkit_Annotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Corpus (코퍼스/말뭉치) \n",
        "- 자연어 연구에 사용되고 형태소 분석을 사전에 시행하여 분석에 정확성을 기한다. 코퍼스 분석은 단어수 계산, n-gram 빈도수와 범워, 키값, 연어(collocation), 의존합자식별(예: 동사-직접목적어 조합) 및 의존합자식별 빈도-범위-연결강도 분석을 한다. \n",
        "\n",
        "- 위의 분석을 위해서는 아래 패키지를 설치한다 \n",
        "-- corpus-toolkit : tokenization(토큰화) & lemmatization(표준형태화) 포함\n",
        "- 참조: Kyle 의 코드에는 Spacy 패키지(tagging(단어에 문법범주 연결) & parsing(구분자를 사용하여 문자열을 구성 요소로 분석))를 포함 하였으나, 여기에서는 Spacy 패키지를 따로 설치하지 않고, nltk에서 사용하는 코드를 사용할 예정.  \n"
      ],
      "metadata": {
        "id": "M7afPLwh-bRv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JBz4z96Z-YV5",
        "outputId": "48995967-fbad-44d1-8107-b486be2a1e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting corpus-toolkit\n",
            "  Downloading corpus_toolkit-0.32-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 14.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: corpus-toolkit\n",
            "Successfully installed corpus-toolkit-0.32\n"
          ]
        }
      ],
      "source": [
        "pip install corpus-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing (사전처리)\n",
        "- 코퍼스 분석을 하려면 분석하려는 텍스트의 단어를 토큰화하여 분석할 수 있는 형태로 준비해 놓아야 한다. i) tokenize 함수를 사용하여 토큰을 리스트화 하여 준비해 놓을 수도 있고, ii) tag 함수를 사용하여 토큰에 문법범주(품사)를 연결해서 준비해 놓을 수도 있다. \n",
        "폴더 형태의 \n",
        "-- [코드문법1] corpus_toolkit패키지에서 corpus_tools모듈을 ct로 줄여서 불러들여라.\n",
        "-- [코드문법2] ct모듈의 ldcorpus( )함수에 brwon_single 데이터 폴더를 인자로 넣어, 그 결과를 brown_corp변수에 할당한다.\n",
        "-- [코드문법3] ct모듈의 tokenize( )함수 인자로 바로 위 코드 변수 brown_corp를 넣어, 그 결과를 tok_corp변수에 할당한다."
      ],
      "metadata": {
        "id": "bO38yLp90864"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IpAwLbK32qeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from corpus_toolkit import corpus_tools as ct\n",
        "brown_corp = ct.ldcorpus(\"brown_single\") #load and read corpus\n",
        "tok_corp = ct.tokenize(brown_corp) #tokenize corpus - by default this lemmatizes as well\n",
        "print(tok_corp)\n",
        "#brown_freq = ct.frequency(tok_corp) #creates a frequency dictionary\n",
        "##note that range can be calculated instead of frequency using the argument calc = \"range\"\n",
        "#ct.head(brown_freq, hits = 10) #print top 10 items"
      ],
      "metadata": {
        "id": "17lsll3VIzu1",
        "outputId": "4372e703-5cd0-4691-a029-539d56bc8ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object tokenize at 0x7f84e8f05ed0>\n"
          ]
        }
      ]
    }
  ]
}